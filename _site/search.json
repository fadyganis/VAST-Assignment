{
  "articles": [
    {
      "path": "about.html",
      "title": "Literature Review",
      "author": [],
      "contents": "\r\n\r\nOn how to tackle the listed objectives, some research was done from the previous VAST challenge that used the same dataset which was in 2014. Of the submissions found and analyzed, the submission conducted by Kevin Griffin of University of California, Davis was chosen as a good foundation on how to move forward.\r\n\r\n\r\nWhat is Relevant for this Report?\r\n\r\n\r\nWhat Griffin did was to analyze the movements of each employee and connect the dots to find meaningful insights relevant to the questions asked. However, Considering the number of employees tracked as well as the dates ranging up to 2 weeks with different times. It will be a challenging task to graph and analyze all movement in that way without a dynamic filtering system that could plot the movements, which is exactly what griffin did in his submission using a custom software. This method would be an ideal solution to the assignment’s 2nd objective if such a dynamic filtering is applicable in the required medium which is R markdown.\r\n\r\n\r\nFrom the movement patterns, insights can also be obtained to how the employees’ relationships are with each other, relating to the 4th objective. While not discussed in the 2014 papers, the acquired information could then be built upon by incorporating a network graph to fully answer the objective.\r\n\r\n\r\nAs for the 3rd objective, the submission’s answer to how the author solved data inconsistencies can be used. The author decided to match the movements of employees with purchase data based on the time similarities. Such a way can also be done to answer this objective.\r\n\r\n\r\nHow to Adapt\r\n\r\n\r\nThis sub-section will mainly focus on how to do the second objective, which is the movement of employees.\r\n\r\n\r\nWhile trying to individually analyze movements of employees is a step in a right direction, doing it in R would be a challenge. Griffin uses a custom-made software that is able to dynamically filter out data from the plot, while such an option is not available for R Markdown.\r\n\r\n\r\nThe thing that comes closest to this would be the library “crosstalk”, which allows a more R shiny-like interaction to filter out data in plots. However, it is still an experimental library with the biggest downside of plotting everything in the dataframe first then filter out what is unneeded. Such a downside would be fine with a dataframe of 100 rows but not ~190000 rows which this project is dealing with.\r\n\r\n\r\nEnter the submission from University of Konstanz. What that team did was to instead reduce the data needed into points where the analysis will only look at the end points of each trip made by the employees. This reduction causes the analysis made from 190000 rows into ~2000, giving more ease into computation power. How to identify the start and end points will be explained in the Methodology section.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T13:05:14+08:00"
    },
    {
      "path": "index.html",
      "title": "Project Introduction",
      "author": [],
      "contents": "\r\n\r\nBackground\r\n\r\n\r\nIn the fictional land of Kronos, the company GAStech has reported missing employees with the environmental activist group known as Protectors of Kronos has been suspected to be behind the disappearances. To help the local authorities, the GPS data of employees as well as purchase historical data of the local businesses using employee loyalty cards and credit cards has been handed over to find leads that might help with the case.\r\n\r\n\r\nAs the data visualization expert called upon to help with the case, this report was created as a result of the investigation. This report documents the objective, literature review, proposed methodology, visualization and insights acquired from the data given.\r\n\r\n\r\nObjective\r\n\r\n\r\nFor this report, the data and questions uses the same data and questions as published in the VAST Data Visualization 2021 - Mini Challenge 2. For the challenge, as said in the background, the main objective is to find any suspicious activities that may relate to the disappearances of the GAStech employees. As such, the objective can then be listed as follow:\r\n\r\n\r\nBy using the credit and loyalty card data, identify the place and time of the most popular locations. In addition, highlight any anomalies found and give suggestions to correct the anomalies.\r\nBy adding the vehicle GPS data, explain if the anomalies found in the first objective change.\r\nInfer which credit card belongs to which employee.\r\nIdentify any informal or unofficial relationships among GAStech employees.\r\nReport any evidence of suspicious activities.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T12:44:24+08:00"
    },
    {
      "path": "Methodology.html",
      "title": "Methodology & Data Preparation",
      "author": [],
      "contents": "\r\n\r\nContents\r\n\r\n\r\n\r\nMethodology\r\n\r\n\r\nWith the objectives sorted out as well as, the way to solve each problem can be planned in advanced by visualizing the following graphs.\r\n\r\n\r\nHeatmap of businesses by each period, this would highlight popular places in a timeframe based on the number of transactions happening.\r\nA map of the region with points showing the start and end point of each trip would give meaning to the popularity of the places as well as could find new leads regarding the routines of each employee.\r\nThe combination of the loyalty card data as well as the routine of the employees acquired from the vehicle data would give a possibility of which card belonging to which employee. However, multiple employees in the same place at the same time would make this impossible.\r\nNetwork graph to show non-professional relationships between employees.\r\nA combination of the above visualizations would be enough to infer any suspicious activities done by the employees. Therefore, no additional visualizations would be needed.\r\n\r\nData Preparation\r\n\r\n\r\nTo start the investigation, the dataset must first be prepared to be able to be properly plotted which will be discussed in this sub-section.\r\n\r\n\r\nTransaction Heatmap\r\n\r\n\r\nTo begin with, this part will utilize the “cc_data” and “loyalty_data” csv files. Some problems found and how it is solved are listed below.\r\n\r\nIncorrect data formats\r\n\r\n\r\ncc <- read_csv(\"_site/Methodology_files/cc_data.csv\")\r\nloy <- read_csv(\"_site/Methodology_files/loyalty_data.csv\")\r\nprint(str(cc))\r\n\r\n\r\n\r\nspec_tbl_df [1,490 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ timestamp : chr [1:1490] \"01/06/2014 07:28\" \"01/06/2014 07:34\" \"01/06/2014 07:35\" \"01/06/2014 07:36\" ...\r\n $ location  : chr [1:1490] \"Brew've Been Served\" \"Hallowed Grounds\" \"Brew've Been Served\" \"Hallowed Grounds\" ...\r\n $ price     : num [1:1490] 11.34 52.22 8.33 16.72 4.24 ...\r\n $ last4ccnum: num [1:1490] 4795 7108 6816 9617 7384 ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   timestamp = col_character(),\r\n  ..   location = col_character(),\r\n  ..   price = col_double(),\r\n  ..   last4ccnum = col_double()\r\n  .. )\r\nNULL\r\n\r\nprint(str(loy))\r\n\r\n\r\nspec_tbl_df [1,392 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ timestamp : chr [1:1392] \"01/06/2014\" \"01/06/2014\" \"01/06/2014\" \"01/06/2014\" ...\r\n $ location  : chr [1:1392] \"Brew've Been Served\" \"Brew've Been Served\" \"Hallowed Grounds\" \"Coffee Shack\" ...\r\n $ price     : num [1:1392] 4.17 9.6 16.53 11.51 12.93 ...\r\n $ loyaltynum: chr [1:1392] \"L2247\" \"L9406\" \"L8328\" \"L6417\" ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   timestamp = col_character(),\r\n  ..   location = col_character(),\r\n  ..   price = col_double(),\r\n  ..   loyaltynum = col_character()\r\n  .. )\r\nNULL\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-07-22T14:39:55+08:00"
    }
  ],
  "collections": []
}

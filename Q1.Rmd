---
title: "Q1"
description: |
  Checking the Popular places.
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(viridis)
library(hrbrthemes)
library(plotly)
library(clock)
library(DT)
cc <- read.csv("_site/cc_data.csv")
cc_loy_clean <- read.csv("_site/cc_loy_clean.csv")
cc_loy_union <- read.csv("_site/cc_loy_union.csv")
loyalty <- read.csv("_site/loyalty_data.csv")


## Extract hour row from cc data

cc$timestamp <- date_time_parse(cc$timestamp,zone='',format = "%m/%d/%Y %H:%M")
cc$H <-  format(cc$timestamp, format='%H')

## Create only mdy column in cc for matching with loyalty

cc$mdy <- format(cc$timestamp, format = "%m/%d/%Y")
cc$mdy <- date_time_parse(cc$mdy,zone = "",format = "%m/%d/%Y")
```

<p>
When analyzing the popularity of locations, the first thing to do is to look at transactions both by date and by hour. Since the loyalty card data does not have an hourly data to extract, the hours will only be shown by the credit card data, while date heatmaps are shown with a combination of both datasets.
</p>

```{r, echo=FALSE}
x <- paste("H",seq(0,23),sep="")
y <- sort(unique(cc$location))
data <- expand.grid(X=x, Y=y)
data$H <- seq(0,23)

for(i in 1:nrow(data)){
  data$Z[i] =  nrow(cc[cc$location == data$Y[i] 
                                 & cc$H == data$H[i],])
}

for(i in 1:nrow(data)){
  if(data$Z[i]==0){
    data$Z[i] <- NA
  }
}

data <- data %>%
  mutate(text = paste0("Hour: ", X, "\n","Location: ", Y, "\n","Number of Transactions: ", Z, "\n"))


p <- ggplot(data, aes(X, Y, fill= Z,text=text)) + 
  geom_tile() +
  scale_fill_viridis() +
  theme_ipsum()
```

```{r,echo=FALSE,layout="1-body-layout",fig.height=7,fig.align='center'}
ggplotly(p,tooltip = "text")
```
```{r, echo=FALSE}
xx <-  sort(unique(cc_loy_union$mdy))
yy <-  sort(unique(cc_loy_union$location))
data2 <- expand.grid(X=xx, Y=yy)
for(i in 1:nrow(data2)){
  data2$Z[i] =  nrow(cc_loy_union[cc_loy_union$location == data2$Y[i] & cc_loy_union$mdy == data2$X[i],])
}

for(i in 1:nrow(data2)){
  if(data2$Z[i]==0){
    data2$Z[i] <- NA
  }
}

data2 <- data2 %>%
  mutate(text = paste0("Date: ", X, "\n","Location: ", Y, "\n","Number of Transactions: ", Z, "\n"))
p2 <- ggplot(data2, aes(X, Y, fill= Z,text=text)) + 
  geom_tile() +
  scale_fill_viridis(discrete=FALSE) +
  theme_ipsum()
```

```{r,echo=FALSE,layout="1-body-outset",fig.height=7,fig.align='center'}
ggplotly(p2,tooltip = "text")
```
<p>
Simply looking at the 2 heatmaps first, some obvious patterns appear which is f&b services busy during mornings, lunch times and evenings. However, there are also some anomalies found which are listed below.
</p>
<p>
1. 	2 weekly transactions in the “Ahaggo Museum” with 2&3 people in one day (11th and 18th January) and 1 person the next (12th and 19th). It is also to note that the solo traveler will always come 1 hour after the group.
</p>
<p>
With the combined dataframes, the revenue acquired from each location can also be checked.
</p>
```{r,echo=FALSE}
x8 <-  sort(unique(cc_loy_union$mdy))
y8 <-  unique(cc_loy_union$location)
data8 <- expand.grid(X=x8, Y=y8)
for(i in 1:nrow(data8)){
  data8$Z[i] =  max(subset(cc_loy_union, location == data8$Y[i] & mdy == data8$X[i])$price)
}

data8$Z[which(data8$Z == -Inf)] <- NA

data8 <- data8 %>%
  mutate(text = paste0("Date: ", X, "\n", "Location: ", Y, "\n","Highest transaction: ", round(Z,2), "\n"))

p8 <- ggplot(data8, aes(X, Y, fill= Z,text=text)) + 
  geom_tile() +
  scale_fill_viridis(discrete=FALSE) +
  theme_ipsum()

plot8 <- ggplotly(p8, tooltip="text")
```

```{r,echo=FALSE,layout="1-body-outset",fig.height=7,fig.align='center'}
ggplotly(p8,tooltip = "text")
```
<p>
Looking at the revenue data, several anomalies can be identified immediately. As then could be investigated by looking at the data involved.
</p>
<p>
2. Biggest red flag, someone spent 10000 on "Frydo's Autosupply and more" on 13th January.
3. Also an anomaly, someone spent 1239 on "Albert's Fine Clothing" on 17th January.
</p>
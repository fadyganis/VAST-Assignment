---
title: "Q1"
description: |
  Checking the Popular places.
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(viridis)
library(hrbrthemes)
library(plotly)
library(clock)
library(DT)
cc <- read.csv("_site/Methodology_files/cc_data.csv")
cc_loy_clean <- read.csv("_site/Methodology_files/cc_loy_clean.csv")
cc_loy_union <- read.csv("_site/Methodology_files/cc_loy_union.csv")
loyalty <- read.csv("_site/Methodology_files/loyalty_data.csv")


## Extract hour row from cc data

cc$timestamp <- date_time_parse(cc$timestamp,zone='',format = "%m/%d/%Y %H:%M")
cc$H <-  format(cc$timestamp, format='%H')

## Create only mdy column in cc for matching with loyalty

cc$mdy <- format(cc$timestamp, format = "%m/%d/%Y")
cc$mdy <- date_time_parse(cc$mdy,zone = "",format = "%m/%d/%Y")
```

<p>
When analyzing the popularity of locations, the first thing to do is to look at transactions both by date and by hour. Since the loyalty card data does not have an hourly data to extract, the hours will only be shown by the credit card data, while date heatmaps are shown with a combination of both datasets.
</p>

```{r, echo=FALSE}
x <- paste("H",seq(0,23),sep="")
y <- sort(unique(cc$location))
data <- expand.grid(X=x, Y=y)
data$H <- seq(0,23)

for(i in 1:nrow(data)){
  data$Z[i] =  nrow(cc[cc$location == data$Y[i] 
                                 & cc$H == data$H[i],])
}

for(i in 1:nrow(data)){
  if(data$Z[i]==0){
    data$Z[i] <- NA
  }
}

data <- data %>%
  mutate(text = paste0("Hour: ", X, "\n","Location: ", Y, "\n","Number of Transactions: ", Z, "\n"))


p <- ggplot(data, aes(X, Y, fill= Z,text=text)) + 
  geom_tile() +
  scale_fill_viridis() +
  theme_ipsum()
```

```{r,echo=FALSE,layout="1-body-layout",fig.height=7,fig.align='center'}
ggplotly(p,tooltip = "text")
```
```{r, echo=FALSE}
xx <-  sort(unique(cc_loy_union$mdy))
yy <-  sort(unique(cc_loy_union$location))
data2 <- expand.grid(X=xx, Y=yy)
for(i in 1:nrow(data2)){
  data2$Z[i] =  nrow(cc_loy_union[cc_loy_union$location == data2$Y[i] & cc_loy_union$mdy == data2$X[i],])
}

for(i in 1:nrow(data2)){
  if(data2$Z[i]==0){
    data2$Z[i] <- NA
  }
}

data2 <- data2 %>%
  mutate(text = paste0("Date: ", X, "\n","Location: ", Y, "\n","Number of Transactions: ", Z, "\n"))
p2 <- ggplot(data2, aes(X, Y, fill= Z,text=text)) + 
  geom_tile() +
  scale_fill_viridis(discrete=FALSE) +
  theme_ipsum()
```

```{r,echo=FALSE,layout="1-body-outset",fig.height=7,fig.align='center'}
ggplotly(p2,tooltip = "text")
```
<p>
Simply looking at the 2 heatmaps first, some obvious patterns appear which is f&b services busy during mornings, lunch times and evenings. However, there are also some anomalies found which are listed below.
</p>
<p>
1. 	2 weekly transactions in the “Ahaggo Museum” with 2&3 people in one day (11th and 18th January) and 1 person the next (12th and 19th). It is also to note that the solo traveler will always come 1 hour after the group.

2. Abila Zacharo has a total of 91 transactions over the dates but only 1 transaction happening at dinner time whereas every other locations are quite busy.
</p>

```{r,echo=FALSE}
abilaZ_filt <- cc_loy_union %>% filter(is.na(H)==TRUE & location == "Abila Zacharo")

abilaZ_filt <- abilaZ_filt[c("location","price","mdy","loyaltynum")]

datatable(abilaZ_filt)

```
<p>
looking at the table, there seems to be some transactions happening using only loyalty cards, and judging by the price it would seem that they are mostly groups with at the 18th a transaction is made by 73.25 of the local currency. As for the timing, they are all done in weekdays so it would be safe to assume they are either during lunchtime or in the evening similar to popular times of other restaurants.
</p>

<p>
With the combined dataframes, the revenue acquired from each location can also be checked.
</p>
```{r,echo=FALSE}
x8 <-  sort(unique(cc_loy_union$mdy))
y8 <-  unique(cc_loy_union$location)
data8 <- expand.grid(X=x8, Y=y8)
for(i in 1:nrow(data8)){
  data8$Z[i] =  max(subset(cc_loy_union, location == data8$Y[i] & mdy == data8$X[i])$price)
}

data8$Z[which(data8$Z == -Inf)] <- NA

data8 <- data8 %>%
  mutate(text = paste0("Date: ", X, "\n", "Location: ", Y, "\n","Highest transaction: ", round(Z,2), "\n"))

p8 <- ggplot(data8, aes(X, Y, fill= Z,text=text)) + 
  geom_tile() +
  scale_fill_viridis(discrete=FALSE) +
  theme_ipsum()

plot8 <- ggplotly(p8, tooltip="text")
```

```{r,echo=FALSE,layout="1-body-outset",fig.height=7,fig.align='center'}
ggplotly(p8,tooltip = "text")
```

<p>
3. Biggest red flag, someone spent 10000 on "Frydo's Autosupply and more" on 13th January.
4. Also an anomaly, someone spent 1239 on "Albert's Fine Clothing" on 17th January.
5. 18th January also saw the largest spending on "General Grocer", which was 477.6. This is an anomaly considering that other max transaction prices are around 270-285 at most.
6. On 13th January, the biggest transaction happening in "Nationwide Refinery" was 204.58. This isn't necessarily a red flag. However, seeing as the regular max transaction revenue can be in the thousands this might be an indicator of a personal purchase instead of corporate.
</p>
<p>
Looking at the revenue data, several anomalies can be identified immediately. As then could be investigated by looking at the data involved.
</p>

```{r,echo=FALSE}
Frydos <- cc_loy_union %>% filter(location == "Frydos Autosupply n' More" & price == 10000)
Frydos <- Frydos[c("location","price","mdy","last4ccnum","loyaltynum")]
Albert <- cc_loy_union %>% filter(location == "Albert's Fine Clothing" & price == 1239.41)
Albert <- Albert[c("location","price","mdy","last4ccnum","loyaltynum")]
GenGroc <- cc_loy_union %>% filter(location == "General Grocer" & price == 477.6)
GenGroc <- GenGroc[c("location","price","mdy","last4ccnum","loyaltynum")]
Natref <- cc_loy_union %>% filter(location == "Nationwide Refinery"&price==204.58)
Natref <- Natref[c("location","price","mdy","last4ccnum","loyaltynum")]
datatable(Frydos)
datatable(Albert)
datatable(GenGroc)
datatable(Natref)

```

<p>
Looking at the data, there doesn't seem to be any noticeable pattern in those anomalies. However, it is important to keep in mind any anomalies to be able to further investigate in subsequent sections.
</p>